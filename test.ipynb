{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Args' object has no attribute 'parameter_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspace/stylespace_plus/test.ipynb Cell 1\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374796c6573706163655f706c7573222c2273657474696e6773223a7b22636f6e74657874223a226d792d72656d6f74652d646f636b65722d6d616368696e65227d7d/workspace/stylespace_plus/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m stylegan_G_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(args\u001b[39m.\u001b[39mpretrained_models_path\u001b[39m.\u001b[39mjoinpath(\u001b[39m\"\u001b[39m\u001b[39mstylegan2-ffhq-256x256.pkl\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374796c6573706163655f706c7573222c2273657474696e6773223a7b22636f6e74657874223a226d792d72656d6f74652d646f636b65722d6d616368696e65227d7d/workspace/stylespace_plus/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m landmarks_model_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(args\u001b[39m.\u001b[39mpretrained_models_path\u001b[39m.\u001b[39mjoinpath(\u001b[39m'\u001b[39m\u001b[39m3DDFA/phase1_wpdc_vdc.pth.tar\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374796c6573706163655f706c7573222c2273657474696e6773223a7b22636f6e74657874223a226d792d72656d6f74652d646f636b65722d6d616368696e65227d7d/workspace/stylespace_plus/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m G \u001b[39m=\u001b[39m Generator(args\u001b[39m=\u001b[39;49margs, id_model_path\u001b[39m=\u001b[39;49mid_model_path, base_generator_path\u001b[39m=\u001b[39;49mstylegan_G_path, landmarks_detector_path\u001b[39m=\u001b[39;49mlandmarks_model_path, \n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374796c6573706163655f706c7573222c2273657474696e6773223a7b22636f6e74657874223a226d792d72656d6f74652d646f636b65722d6d616368696e65227d7d/workspace/stylespace_plus/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m           device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374796c6573706163655f706c7573222c2273657474696e6773223a7b22636f6e74657874223a226d792d72656d6f74652d646f636b65722d6d616368696e65227d7d/workspace/stylespace_plus/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m G\u001b[39m.\u001b[39m_load(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374796c6573706163655f706c7573222c2273657474696e6773223a7b22636f6e74657874223a226d792d72656d6f74652d646f636b65722d6d616368696e65227d7d/workspace/stylespace_plus/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m fid_score \u001b[39m=\u001b[39m FIDScore(args, G, device)\n",
      "File \u001b[0;32m/workspace/stylespace_plus/model/generator.py:49\u001b[0m, in \u001b[0;36mGenerator.__init__\u001b[0;34m(self, args, id_model_path, base_generator_path, landmarks_detector_path, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlandmarks_detector\u001b[39m.\u001b[39m_test()\n\u001b[1;32m     40\u001b[0m \u001b[39m# Initialize reference autoencoder\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# \u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# self.reference_pose_encoder = reference_autoencoder.ReferenceEncoder(args, 3).to(device)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[39m# Initialize orthogonal nn.Parameter\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39;49mparameter_embedding:\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpose_basis \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39morthogonal_(nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mrandn(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m)))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpose_parameter_decoder \u001b[39m=\u001b[39m parameter_decoder\u001b[39m.\u001b[39mParameterDecoder(args, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Args' object has no attribute 'parameter_embedding'"
     ]
    }
   ],
   "source": [
    "from metrics.frechet_inception_distance import FIDScore\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from model.generator import Generator\n",
    "\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.name = \"exp_11\"\n",
    "        self.resolution = 256\n",
    "        self.load_checkpoint = True\n",
    "        self.train = False\n",
    "        self.dataset_path = Path(\"./dataset\")\n",
    "        self.results_dir = Path(\"./output\")\n",
    "        self.pretrained_models_path = Path(\"./pretrained\")\n",
    "        self.train_data_size = 50000\n",
    "        self.batch_size = 6\n",
    "        self.reals = False\n",
    "        self.test_real_attr = True\n",
    "        self.train_real_attr = False\n",
    "        self.weights_dir = self.results_dir.joinpath(self.name+\"/weights\")\n",
    "        self.cache = True\n",
    "        self.feature_embedding = False\n",
    "\n",
    "\n",
    "args = Args()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "id_model_path = str(args.pretrained_models_path.joinpath(\n",
    "    \"20180402-114759-vggface2.pt\"))\n",
    "stylegan_G_path = str(args.pretrained_models_path.joinpath(\n",
    "    \"stylegan2-ffhq-256x256.pkl\"))\n",
    "landmarks_model_path = str(\n",
    "    args.pretrained_models_path.joinpath('3DDFA/phase1_wpdc_vdc.pth.tar'))\n",
    "\n",
    "G = Generator(args=args, id_model_path=id_model_path, base_generator_path=stylegan_G_path, landmarks_detector_path=landmarks_model_path,\n",
    "              device=device)\n",
    "\n",
    "G._load(\"\")\n",
    "\n",
    "fid_score = FIDScore(args, G, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = fid_score.calculate_fid(max_real=None, num_gen=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.889627031080321"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import landmarks_detector, networks_stylegan2, id_encoder, attr_encoder, reference_autoencoder, reference_network\n",
    "from general_utils import legacy\n",
    "import torchvision.transforms.functional as TF\n",
    "from data_loader.gen_data import GeneratedDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import dnnlib\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.name = \"exp_07\"\n",
    "        self.resolution = 256\n",
    "        self.load_checkpoint = True\n",
    "        self.train = False\n",
    "        self.dataset_path = Path(\"./dataset\")\n",
    "        self.results_dir = Path(\"./output\")\n",
    "        self.pretrained_models_path = Path(\"./pretrained\")\n",
    "        self.train_data_size = 50000\n",
    "        self.batch_size = 6\n",
    "        self.reals = False\n",
    "        self.test_real_attr = True\n",
    "        self.train_real_attr = False\n",
    "        self.weights_dir = self.results_dir.joinpath(self.name+\"/weights\")\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def get_id_attr_sampler(dataset_length, cross_frequency):\n",
    "    split = dataset_length // (cross_frequency + 1)\n",
    "    id_dataset_length = dataset_length - split\n",
    "    indices = list(range(dataset_length))\n",
    "    random.shuffle(indices)\n",
    "    id_sampler = SubsetRandomSampler(indices[split:])\n",
    "    attr_sampler = SubsetRandomSampler(indices[:split])\n",
    "    return id_sampler, attr_sampler, id_dataset_length\n",
    "\n",
    "\n",
    "id_model_path = str(args.pretrained_models_path.joinpath(\n",
    "    \"20180402-114759-vggface2.pt\"))\n",
    "stylegan_G_path = str(args.pretrained_models_path.joinpath(\n",
    "    \"stylegan2-ffhq-256x256.pkl\"))\n",
    "landmarks_detector_path = str(\n",
    "    args.pretrained_models_path.joinpath('3DDFA/phase1_wpdc_vdc.pth.tar'))\n",
    "\n",
    "# Load style generator and freeze its parameters\n",
    "with dnnlib.util.open_url(stylegan_G_path) as f:\n",
    "    data = legacy.load_network_pkl(f)\n",
    "    G = data[\"G_ema\"].to(device)\n",
    "\n",
    "stylegan_generator = networks_stylegan2.Generator(**G.init_kwargs).to(device)\n",
    "stylegan_generator.load_state_dict(G.state_dict())\n",
    "stylegan_generator.eval()\n",
    "\n",
    "for param in stylegan_generator.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Load identity encoder and freeze its parameters\n",
    "_id_encoder = id_encoder.ID_Encoder(args, id_model_path).to(device)\n",
    "_id_encoder._test()\n",
    "\n",
    "# Inintialize attribute encoder\n",
    "_attr_encoder = attr_encoder.Attr_Encoder_Deprecated(args).to(device)\n",
    "\n",
    "# Load landmarks detector and freeze its parameters\n",
    "_landmarks_detector = landmarks_detector.LandmarksDetector(\n",
    "    args, landmarks_detector_path).to(device)\n",
    "_landmarks_detector._test()\n",
    "\n",
    "_reference_network = reference_network.ReferenceNetwork(args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader.ffhq_data import FFHQDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "ffhq_dataset = FFHQDataset(args)\n",
    "ffhq_dataloader = DataLoader(ffhq_dataset, batch_size=6, pin_memory=True)\n",
    "ffhq_iter = iter(ffhq_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = next(ffhq_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "id_image = cv.imread(\"./dataset/gen_dataset/image/1000/3.png\")\n",
    "id_image = torch.from_numpy(id_image.transpose(\n",
    "    (2, 0, 1))).float().flip(-3).to(device)\n",
    "id_image.sub_(127.5).div_(128)\n",
    "attr_image = cv.imread(\"./dataset/gen_dataset/image/1000/1.png\")\n",
    "attr_image = torch.from_numpy(attr_image.transpose(\n",
    "    (2, 0, 1))).float().flip(-3).to(device)\n",
    "attr_image.sub_(127.5).div_(128)\n",
    "\n",
    "id_image = torch.broadcast_to(id_image, (6, *id_image.shape))\n",
    "attr_image = torch.broadcast_to(attr_image, (6, *attr_image.shape))\n",
    "\n",
    "id_embd = _id_encoder(id_image)\n",
    "attr_embd = _attr_encoder(attr_image)\n",
    "\n",
    "control_vector = _reference_network(attr_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.from_numpy(np.random.RandomState(3).randn(512)).to(device)\n",
    "z = torch.broadcast_to(z, (6, *z.shape))\n",
    "\n",
    "gen_images = stylegan_generator(z, control_vector)\n",
    "pts68, pose, idx_list = _landmarks_detector(gen_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
