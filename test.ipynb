{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, dnnlib\n",
    "from general_utils import legacy\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "\n",
    "from model import networks_stylegan2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with dnnlib.util.open_url(\"./pretrained/stylegan2-ffhq-256x256.pkl\") as f:\n",
    "    data = legacy.load_network_pkl(f)\n",
    "    G = data[\"G_ema\"].to(device)\n",
    "\n",
    "stylegan_generator = networks_stylegan2.Generator(**G.init_kwargs).to(device)\n",
    "stylegan_generator.load_state_dict(G.state_dict())\n",
    "stylegan_generator.eval()\n",
    "\n",
    "label = torch.zeros([1, G.c_dim], device=device)\n",
    "z = torch.from_numpy(np.random.RandomState(0).randn(1, G.z_dim)).to(device)\n",
    "ctrlv = torch.zeros([1, 7424], device=device)\n",
    "img = stylegan_generator(z, label, ctrlv)\n",
    "img = (img * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from data_loader.gen_data import GeneratedDataset\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "from model.network import Network\n",
    "from model.generator import Generator\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from writer import Writer\n",
    "from trainer import Trainer\n",
    "from general_utils import arglib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.name = \"exp01\"\n",
    "        self.resolution = 256\n",
    "        self.load_checkpoint = False\n",
    "        self.train = True\n",
    "        self.dataset_path = Path(\"./dataset\")\n",
    "        self.results_dir = Path(\"./output/\")\n",
    "        self.train_data_size = 50000\n",
    "        self.batch_size = 6\n",
    "        self.reals = False\n",
    "        self.test_real_attr = True\n",
    "        self.train_real_attr = False\n",
    "        self.weights_dir = Path(\"./output/exp01/weights\")\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def get_id_attr_sampler(dataset_length, cross_frequency):\n",
    "    split = dataset_length // (cross_frequency + 1)\n",
    "    indices = list(range(dataset_length))\n",
    "    random.shuffle(indices)\n",
    "    id_sampler = SubsetRandomSampler(indices[split:])\n",
    "    attr_sampler = SubsetRandomSampler(indices[:split])\n",
    "    return id_sampler, attr_sampler\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Writer.set_writer(args.results_dir)\n",
    "\n",
    "id_model_path = \"./pretrained/resnet50_scratch_weight.pkl\"\n",
    "stylegan_G_path = \"./pretrained/ffhq.pkl\"\n",
    "landmarks_model_path = \"./pretrained/3DDFA/phase1_wpdc_vdc.pth.tar\"\n",
    "\n",
    "network = Network(args=args, id_model_path=id_model_path, base_generator_path=stylegan_G_path,\n",
    "                  landmark_model_path=landmarks_model_path, device=DEVICE)\n",
    "# Dataset\n",
    "train_dataset = GeneratedDataset(args, \"train\")\n",
    "train_dataset_length = len(train_dataset)\n",
    "train_id_sampler, train_attr_sampler = get_id_attr_sampler(train_dataset_length, 3)\n",
    "train_id_loader = DataLoader(train_dataset, batch_size=6, sampler=train_id_sampler)\n",
    "train_attr_loader = DataLoader(train_dataset, batch_size=6, sampler=train_attr_sampler)\n",
    "\n",
    "id_images, id_zs = next(iter(train_id_loader))\n",
    "attr_images = id_images\n",
    "\n",
    "id_images = id_images.to(DEVICE)\n",
    "id_zs = id_zs.to(DEVICE)\n",
    "attr_images = attr_images.to(DEVICE)\n",
    "\n",
    "# Identity embedding and attribute embedding\n",
    "id_embedding = network.generator.id_encoder(id_images)\n",
    "attr_embedding = network.generator.attr_encoder(attr_images)\n",
    "\n",
    "# Attribute landmarks\n",
    "attr_landmarks, attr_idx_list = network.generator.landmarks_detector(attr_images)\n",
    "\n",
    "# Style+ embedding and control vector generation\n",
    "feature_tag = torch.concat([id_embedding, attr_embedding], -1)\n",
    "sp_embedding = network.generator.reference_encoder(feature_tag)\n",
    "control_vector = network.generator.reference_decoder(sp_embedding)\n",
    "\n",
    "gen_images = network.generator.stylegan_generator(id_zs, control_vector)\n",
    "\n",
    "gen_images = F.resize(gen_images, (256, 256))\n",
    "\n",
    "# Convert RGB to BGR to make the generated image compatible with the landmark detector\n",
    "gen_images = gen_images.flip(-3)\n",
    "\n",
    "pred_landmarks, gen_idx_list = network.generator.landmarks_detector(gen_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = pred_landmarks - attr_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(network.generator.reference_decoder.parameters())[-1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(gen_idx_list) > len(attr_idx_list):\n",
    "    for e in gen_idx_list:\n",
    "        if e not in attr_idx_list:\n",
    "            gen_images = gen_images.narrow(0, e, 1).squeeze()\n",
    "else:\n",
    "    for e in attr_idx_list:\n",
    "        if e not in gen_idx_list:\n",
    "            attr_images = attr_images.narrow(e, 1, 1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_utils import general_utils\n",
    "general_utils.save_images(id_images, attr_images, gen_images, attr_landmarks, pred_landmarks, 256, \"./01.png\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img_ori = attr_images[1].detach().cpu().permute(1,2,0)\n",
    "img_ori = (img_ori * 127.5 + 128).to(torch.uint8).clamp(0, 255).numpy()\n",
    "pts68 = attr_landmarks[0].cpu().detach()\n",
    "height, width = img_ori.shape[:2]\n",
    "plt.imshow(img_ori[:,:,::-1])\n",
    "#plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "plt.axis('off')\n",
    "\n",
    "style = \"simple\"\n",
    "\n",
    "if not type(pts68) in [tuple, list]:\n",
    "        pts = [pts68]\n",
    "for i in range(len(pts)):\n",
    "    if style == 'simple':\n",
    "        plt.plot(pts[i][0, :], pts[i][1, :], 'o', markersize=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img_ori = gen_images[1].detach().cpu().permute(1,2,0)\n",
    "img_ori = (img_ori * 127.5 + 128).to(torch.uint8).clamp(0, 255).numpy()\n",
    "pts68 = pred_landmarks[1].cpu().detach()\n",
    "height, width = img_ori.shape[:2]\n",
    "plt.imshow(img_ori[:,:,::-1])\n",
    "#plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "plt.axis('off')\n",
    "\n",
    "style = \"simple\"\n",
    "\n",
    "if not type(pts68) in [tuple, list]:\n",
    "        pts = [pts68]\n",
    "for i in range(len(pts)):\n",
    "    if style == 'simple':\n",
    "        plt.plot(pts[i][0, :], pts[i][1, :], 'o', markersize=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import kornia as K\n",
    "import torchvision.transforms.functional as F\n",
    "from kornia.contrib import FaceDetector, FaceDetectorResult, FaceKeypoint\n",
    "\n",
    "\n",
    "def scale_image(img: np.ndarray, size: int) -> np.ndarray:\n",
    "    h, w = img.shape[:2]\n",
    "    scale = 1.0 * size / w\n",
    "    return cv2.resize(img, (int(w * scale), int(h * scale)))\n",
    "\n",
    "\n",
    "img_raw = cv2.imread(\"./dataset/gen_dataset/data/1000/0.png\", cv2.IMREAD_COLOR)\n",
    "img_raw = scale_image(img_raw, 320)\n",
    "img_vis = img_raw.copy()\n",
    "\n",
    "img = K.image_to_tensor(img_raw, keepdim=False).cuda()\n",
    "img = K.color.bgr_to_rgb(img.float())\n",
    "face_detector = FaceDetector().cuda()\n",
    "\n",
    "dets = face_detector(img)\n",
    "dets = [FaceDetectorResult(o) for o in dets[0]]\n",
    "\n",
    "\n",
    "def draw_keypoint(img: np.ndarray, det: FaceDetectorResult, kpt_type: FaceKeypoint) -> np.ndarray:\n",
    "    kpt = tuple(det.get_keypoint(kpt_type).int().tolist())\n",
    "    return cv2.circle(img, kpt, 2, (255, 0, 0), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in dets:\n",
    "    if b.score < 0.8:\n",
    "        continue\n",
    "\n",
    "    print(b.top_left.int().tolist(), b.bottom_right.int().tolist())\n",
    "\n",
    "    img_vis = cv2.rectangle(img_vis, tuple(b.top_left.int().tolist()), tuple(\n",
    "        b.bottom_right.int().tolist()), (0, 255, 0), 4)\n",
    "\n",
    "    img_vis = draw_keypoint(img_vis, b, FaceKeypoint.EYE_LEFT)\n",
    "    img_vis = draw_keypoint(img_vis, b, FaceKeypoint.EYE_RIGHT)\n",
    "    img_vis = draw_keypoint(img_vis, b, FaceKeypoint.NOSE)\n",
    "    img_vis = draw_keypoint(img_vis, b, FaceKeypoint.MOUTH_LEFT)\n",
    "    img_vis = draw_keypoint(img_vis, b, FaceKeypoint.MOUTH_RIGHT)\n",
    "\n",
    "    # draw the text score\n",
    "    cx = int(b.xmin)\n",
    "    cy = int(b.ymin + 12)\n",
    "    img_vis = cv2.putText(\n",
    "        img_vis, f\"{b.score:.2f}\", (cx, cy), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "\n",
    "cv2.imwrite(\"./test.png\", img_vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in G.attr_encoder.parameters():\n",
    "    print(p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.attr_encoder(torch.randn((6, 3, 256, 256), requires_grad=True)).requires_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader.gen_data import FaceLandmarksDataset, Transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = FaceLandmarksDataset(\"train\", transform=Transforms())\n",
    "train_loader = DataLoader(train_dataset, 6, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_images, ref_landmarks, ref_zs, ref_bboxs = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.id_encoder import ID_Encoder\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.resolution = 256\n",
    "        self.load_checkpoint = False\n",
    "        self.train = True\n",
    "        self.dataset_path = Path(\"./dataset\")\n",
    "        self.train_data_size = 50000\n",
    "        self.batch_size = 6\n",
    "        self.reals = False\n",
    "        self.test_real_attr = True\n",
    "        self.train_real_attr = False\n",
    "\n",
    "\n",
    "args = Args()\n",
    "id_encoder = ID_Encoder(args, \"./pretrained/resnet50_scratch_weight.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_encoder(torch.randn((6, 3, 256, 256))).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_encoder._train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in id_encoder.base_model.named_parameters():\n",
    "    print(name, param.requires_grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
