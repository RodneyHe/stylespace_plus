{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /home/zhuo/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
      "100%|██████████| 104M/104M [00:01<00:00, 108MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttrEncoder loads checkpoint from: output/exp_27-conv3--8/weights/AttrEncoder.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sequential:\n\tsize mismatch for 6.weight: copying a param with shape torch.Size([2816, 4096]) from checkpoint, the shape in current model is torch.Size([4928, 4096]).\n\tsize mismatch for 6.bias: copying a param with shape torch.Size([2816]) from checkpoint, the shape in current model is torch.Size([4928]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 99\u001b[0m\n\u001b[1;32m     93\u001b[0m landmarks_model_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m     94\u001b[0m     args\u001b[39m.\u001b[39mpretrained_models_path\u001b[39m.\u001b[39mjoinpath(\u001b[39m'\u001b[39m\u001b[39m3DDFA/phase1_wpdc_vdc.pth.tar\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     96\u001b[0m network \u001b[39m=\u001b[39m Network(args\u001b[39m=\u001b[39margs, id_model_path\u001b[39m=\u001b[39mid_model_path, base_generator_path\u001b[39m=\u001b[39mstylegan_G_path,\n\u001b[1;32m     97\u001b[0m                     landmarks_detector_path\u001b[39m=\u001b[39mlandmarks_model_path, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> 99\u001b[0m network\u001b[39m.\u001b[39m_load(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m zero_ctrlv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m3\u001b[39m, \u001b[39m4928\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    102\u001b[0m style_padding1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m3\u001b[39m, \u001b[39m1536\u001b[39m))\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Developer/stylespace_plus/model/network.py:33\u001b[0m, in \u001b[0;36mNetwork._load\u001b[0;34m(self, reason)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load\u001b[39m(\u001b[39mself\u001b[39m, reason\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator\u001b[39m.\u001b[39m_load(reason)\n",
      "File \u001b[0;32m~/Developer/stylespace_plus/model/generator.py:169\u001b[0m, in \u001b[0;36mGenerator._load\u001b[0;34m(self, reason)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpression_parameter_decoder\u001b[39m.\u001b[39m_load(\u001b[39m\"\u001b[39m\u001b[39m_expression\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mreason)\n\u001b[1;32m    168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreference_network\u001b[39m.\u001b[39m_load(reason)\n",
      "File \u001b[0;32m~/Developer/stylespace_plus/model/reference_network.py:41\u001b[0m, in \u001b[0;36mReferenceNetwork._load\u001b[0;34m(self, reason)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load\u001b[39m(\u001b[39mself\u001b[39m, reason):\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mweights_dir\u001b[39m.\u001b[39mjoinpath(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m+\u001b[39m reason \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m))))\n\u001b[1;32m     42\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m     43\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m load checkpoint from: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mweights_dir\u001b[39m.\u001b[39mjoinpath(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m+\u001b[39m reason \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sequential:\n\tsize mismatch for 6.weight: copying a param with shape torch.Size([2816, 4096]) from checkpoint, the shape in current model is torch.Size([4928, 4096]).\n\tsize mismatch for 6.bias: copying a param with shape torch.Size([2816]) from checkpoint, the shape in current model is torch.Size([4928])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dlib\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "from data_loader.ffhq_data import FFHQDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from general_utils.landmarks_utils import parse_roi_box_from_bbox\n",
    "\n",
    "from pathlib import Path\n",
    "from model.network import Network\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.name = \"exp_27-conv3--8\"\n",
    "        self.resolution = 256\n",
    "        self.load_checkpoint = False\n",
    "        self.train = False\n",
    "        self.dataset_path = Path(\"./dataset\")\n",
    "        self.results_dir = Path(\"./output\")\n",
    "        self.pretrained_models_path = Path(\"./pretrained\")\n",
    "        self.train_data_size = 50000\n",
    "        self.batch_size = 6\n",
    "        self.reals = False\n",
    "        self.test_real_attr = True\n",
    "        self.train_real_attr = False\n",
    "        self.weights_dir = self.results_dir.joinpath(self.name+\"/weights\")\n",
    "        self.cache = True\n",
    "        self.parameter_embedding = False\n",
    "\n",
    "\n",
    "args = Args()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ffhq_dataset = FFHQDataset(args)\n",
    "real_dataset_dir = os.path.join(args.dataset_path, \"ffhq256_dataset\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ffhq_dataloader = DataLoader(ffhq_dataset, batch_size=5, pin_memory=True)\n",
    "ffhq_iter = iter(ffhq_dataloader)\n",
    "\n",
    "attr_path_list = [\"./dataset/ffhq256_dataset/image/07000/07983.png\",\n",
    "                  \"./dataset/ffhq256_dataset/image/04000/04986.png\",\n",
    "                  \"./dataset/ffhq256_dataset/image/07000/07024.png\"]\n",
    "\n",
    "attr_list = []\n",
    "for attr_path in attr_path_list:\n",
    "    attr_image = cv.imread(attr_path)\n",
    "\n",
    "    attr_image = torch.from_numpy(attr_image.transpose((2, 0, 1))).float()\n",
    "    attr_image.sub_(127.5).div_(128)\n",
    "\n",
    "    attr_image = attr_image.flip(-3)  # convert to RGB\n",
    "    attr_list.append(attr_image[None, ...])\n",
    "\n",
    "attr_images = torch.cat(attr_list, 0).to(device)\n",
    "\n",
    "id_path_list = [\"./dataset/gen_dataset/image/14000/13006.png\",\n",
    "                \"dataset/gen_dataset/image/43000/42024.png\",\n",
    "                \"dataset/gen_dataset/image/46000/45009.png\"]\n",
    "\n",
    "id_image_list = []\n",
    "id_seed_list = []\n",
    "for id_path in id_path_list:\n",
    "    id_image = cv.imread(id_path)\n",
    "\n",
    "    id_image = torch.from_numpy(id_image.transpose((2, 0, 1))).float()\n",
    "    id_image.sub_(127.5).div_(128)\n",
    "\n",
    "    id_image = id_image.flip(-3)  # convert to RGB\n",
    "    id_image_list.append(id_image[None, ...])\n",
    "\n",
    "    id_image_seed = int(os.path.splitext(os.path.basename(id_path))[0])\n",
    "    id_seed_list.append(id_image_seed)\n",
    "\n",
    "id_images = torch.cat(id_image_list, 0).to(device)\n",
    "\n",
    "id_model_path = str(args.pretrained_models_path.joinpath(\n",
    "    \"20180402-114759-vggface2.pt\"))\n",
    "stylegan_G_path = str(args.pretrained_models_path.joinpath(\n",
    "    \"stylegan2-ffhq-256x256.pkl\"))\n",
    "landmarks_model_path = str(\n",
    "    args.pretrained_models_path.joinpath('3DDFA/phase1_wpdc_vdc.pth.tar'))\n",
    "\n",
    "network = Network(args=args, id_model_path=id_model_path, base_generator_path=stylegan_G_path,\n",
    "                    landmarks_detector_path=landmarks_model_path, device=device)\n",
    "\n",
    "network._load(\"\")\n",
    "\n",
    "zero_ctrlv = torch.zeros((3, 4928)).to(device)\n",
    "style_padding1 = torch.zeros((3, 1536)).to(device)\n",
    "style_padding2 = torch.zeros((3, 576)).to(device)\n",
    "\n",
    "zs_list = []\n",
    "for id_seed in id_seed_list:\n",
    "    z = torch.from_numpy(np.random.RandomState(id_seed).randn(1, 512)).to(device)\n",
    "    zs_list.append(z)\n",
    "\n",
    "zs = torch.cat(zs_list, 0)\n",
    "\n",
    "ws = network.generator.stylegan_generator.mapping(zs, 0)\n",
    "\n",
    "attr_lnds_result = network.generator.landmarks_detector(attr_images.flip(-3))\n",
    "\n",
    "attr_lnds, attr_pose = attr_lnds_result[0], attr_lnds_result[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    attr_embeddings = network.generator.attr_encoder(attr_images)\n",
    "\n",
    "    control_vectors = network.generator.reference_network(attr_embeddings)\n",
    "    control_vectors = torch.cat([style_padding1, control_vectors, style_padding2], -1)\n",
    "    # control_vector = torch.cat([control_vector, style_padding], -1)\n",
    "\n",
    "    gen_images = network.generator.stylegan_generator.synthesis(ws, control_vectors)\n",
    "\n",
    "\n",
    "\n",
    "    id_images = ((id_images + 1) / 2).clamp(0, 1)\n",
    "    attr_images = ((attr_images + 1) / 2).clamp(0, 1)\n",
    "    gen_images = ((gen_images + 1) / 2).clamp(0, 1)\n",
    "\n",
    "    gen_images = torch.cat([gen_image for gen_image in gen_images], 2)\n",
    "\n",
    "    attr_images = torch.cat([attr_image for attr_image in attr_images], 2)\n",
    "\n",
    "    gen_results = torch.cat([id_images[0], gen_images], 2)\n",
    "\n",
    "    attr_images = torch.cat([torch.ones(3, 256, 256).to(device), attr_images], 2)\n",
    "    gen_results = torch.cat([attr_images, gen_results], 1)\n",
    "\n",
    "TF.to_pil_image(gen_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dlib\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "from data_loader.ffhq_data import FFHQDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from general_utils.landmarks_utils import parse_roi_box_from_bbox\n",
    "\n",
    "from pathlib import Path\n",
    "from model.network import Network\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.name = \"exp_27-conv3--8\"\n",
    "        self.resolution = 256\n",
    "        self.load_checkpoint = False\n",
    "        self.train = False\n",
    "        self.dataset_path = Path(\"./dataset\")\n",
    "        self.results_dir = Path(\"./output\")\n",
    "        self.pretrained_models_path = Path(\"./pretrained\")\n",
    "        self.train_data_size = 50000\n",
    "        self.batch_size = 6\n",
    "        self.reals = False\n",
    "        self.test_real_attr = True\n",
    "        self.train_real_attr = False\n",
    "        self.weights_dir = self.results_dir.joinpath(self.name+\"/weights\")\n",
    "        self.cache = True\n",
    "        self.parameter_embedding = False\n",
    "\n",
    "\n",
    "args = Args()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ffhq_dataset = FFHQDataset(args)\n",
    "real_dataset_dir = os.path.join(args.dataset_path, \"ffhq256_dataset\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ffhq_dataloader = DataLoader(ffhq_dataset, batch_size=5, pin_memory=True)\n",
    "ffhq_iter = iter(ffhq_dataloader)\n",
    "\n",
    "attr_path_list = [\"./dataset/ffhq256_dataset/image/07000/07983.png\",\n",
    "                  \"./dataset/ffhq256_dataset/image/04000/04986.png\",\n",
    "                  \"./dataset/ffhq256_dataset/image/07000/07024.png\"]\n",
    "\n",
    "attr_list = []\n",
    "for attr_path in attr_path_list:\n",
    "    attr_image = cv.imread(attr_path)\n",
    "\n",
    "    attr_image = torch.from_numpy(attr_image.transpose((2, 0, 1))).float()\n",
    "    attr_image.sub_(127.5).div_(128)\n",
    "\n",
    "    attr_image = attr_image.flip(-3)  # convert to RGB\n",
    "    attr_list.append(attr_image[None, ...])\n",
    "\n",
    "attr_images = torch.cat(attr_list, 0).to(device)\n",
    "\n",
    "id_path_list = [\"./dataset/gen_dataset/image/14000/13006.png\",\n",
    "                \"dataset/gen_dataset/image/43000/42024.png\",\n",
    "                \"dataset/gen_dataset/image/46000/45009.png\"]\n",
    "\n",
    "id_image_list = []\n",
    "id_seed_list = []\n",
    "for id_path in id_path_list:\n",
    "    id_image = cv.imread(id_path)\n",
    "\n",
    "    id_image = torch.from_numpy(id_image.transpose((2, 0, 1))).float()\n",
    "    id_image.sub_(127.5).div_(128)\n",
    "\n",
    "    id_image = id_image.flip(-3)  # convert to RGB\n",
    "    id_image_list.append(id_image[None, ...])\n",
    "\n",
    "    id_image_seed = int(os.path.splitext(os.path.basename(id_path))[0])\n",
    "    id_seed_list.append(id_image_seed)\n",
    "\n",
    "id_images = torch.cat(id_image_list, 0).to(device)\n",
    "\n",
    "# id_image_path = \"gen_dataset/image/3000/2641.png\"\n",
    "# id_image_seed = int(os.path.splitext(os.path.basename(id_image_path))[0])\n",
    "# id_image = cv.imread(str(args.dataset_path.joinpath(id_image_path)))\n",
    "# id_image = torch.from_numpy(id_image.transpose(\n",
    "#     (2, 0, 1))).float().to(device).flip(-3)\n",
    "# id_image.sub_(127.5).div_(128)\n",
    "\n",
    "id_images = torch.broadcast_to(id_image, [3, *id_image.shape])\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\n",
    "    \"./pretrained/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "\n",
    "def generate_image(args, id_images, id_seed_list, attr_images):\n",
    "\n",
    "    id_model_path = str(args.pretrained_models_path.joinpath(\n",
    "        \"20180402-114759-vggface2.pt\"))\n",
    "    stylegan_G_path = str(args.pretrained_models_path.joinpath(\n",
    "        \"stylegan2-ffhq-256x256.pkl\"))\n",
    "    landmarks_model_path = str(\n",
    "        args.pretrained_models_path.joinpath('3DDFA/phase1_wpdc_vdc.pth.tar'))\n",
    "\n",
    "    network = Network(args=args, id_model_path=id_model_path, base_generator_path=stylegan_G_path,\n",
    "                      landmarks_detector_path=landmarks_model_path, device=device)\n",
    "\n",
    "    network._load(\"\")\n",
    "\n",
    "    zero_ctrlv = torch.zeros((1, 4928)).to(device)\n",
    "    style_padding1 = torch.zeros((1, 2560)).to(device)\n",
    "    style_padding2 = torch.zeros((1, 576)).to(device)\n",
    "\n",
    "    identity_score = 0\n",
    "    expression_score = 0\n",
    "    pose_score = 0\n",
    "    #pbar = tqdm(range(1), ncols=80)\n",
    "    for _ in attr_images:\n",
    "        #z = torch.randn((1, 512)).to(device).clamp(-1, 1)\n",
    "        z = torch.from_numpy(np.random.RandomState(id_img_seed).randn(1, 512)).to(device)\n",
    "        ws = network.generator.stylegan_generator.mapping(\n",
    "            torch.broadcast_to(z, (5, 512)), 0)\n",
    "\n",
    "        # attr_images = next(ffhq_iter)\n",
    "        # attr_images = attr_images.to(device)\n",
    "\n",
    "        attr_lnds_result = network.generator.landmarks_detector(\n",
    "            attr_images.flip(-3))\n",
    "\n",
    "        if attr_lnds_result is None:\n",
    "            continue\n",
    "\n",
    "        attr_lnds, attr_pose = attr_lnds_result[0], attr_lnds_result[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen_id_image = network.generator.stylegan_generator.synthesis(\n",
    "                ws, zero_ctrlv)\n",
    "\n",
    "            gen_id_embedding = network.generator.id_encoder(gen_id_image)\n",
    "            attr_embedding = network.generator.attr_encoder(attr_images)\n",
    "\n",
    "            # feature_input = torch.concat([gen_id_embedding, attr_embedding], -1)\n",
    "            feature_input = attr_embedding\n",
    "\n",
    "            control_vector = network.generator.reference_network(feature_input)\n",
    "\n",
    "            #control_vector = torch.cat([style_padding1, control_vector, style_padding2], -1)\n",
    "\n",
    "            # control_vector = torch.cat([control_vector, style_padding], -1)\n",
    "\n",
    "            gen_images = network.generator.stylegan_generator.synthesis(\n",
    "                ws, control_vector)\n",
    "\n",
    "            gen_embedding = network.generator.id_encoder(gen_images)\n",
    "\n",
    "            # identity_score += nn.functional.cosine_similarity(\n",
    "            #     gen_id_embedding, gen_embedding).mean().item()\n",
    "\n",
    "            # current_identity_score = nn.functional.cosine_similarity(\n",
    "            #     gen_id_embedding, gen_embedding)\n",
    "            # identity_score += torch.pow((current_identity_score -\n",
    "            #                             0.84), 2).item()\n",
    "\n",
    "            # gen_lnds_result = network.generator.landmarks_detector(\n",
    "            #     gen_images.flip(-3))\n",
    "\n",
    "            # if gen_lnds_result is None:\n",
    "            #     continue\n",
    "\n",
    "            #gen_lnds, gen_pose = gen_lnds_result[0], gen_lnds_result[1]\n",
    "\n",
    "            # print(torch.pow((gen_lnds[0]/256 - attr_lnds[0]/256), 2).sum())\n",
    "\n",
    "            # expression_score += torch.pow(\n",
    "            #     (gen_lnds[0]/256 - attr_lnds[0]/256), 2).sum().item()\n",
    "            # pose_score += torch.pow((gen_pose[0] -\n",
    "            #                         attr_pose[0]), 2).sum().item()\n",
    "\n",
    "            # current_expression_score = torch.pow(\n",
    "            #     (gen_lnds[0]/256 - attr_lnds[0]/256), 2).sum()\n",
    "            # expression_score += torch.pow(\n",
    "            #     (current_expression_score - 0.026), 2).item()\n",
    "            # current_pose_score = torch.pow(\n",
    "            #     (gen_pose[0] - attr_pose[0]), 2).sum()\n",
    "            # pose_score += torch.pow((current_pose_score -\n",
    "            #                         0.05741964), 2).item()\n",
    "\n",
    "            id_images = ((gen_id_image + 1) / 2).clamp(0, 1)\n",
    "            attr_images = ((attr_images + 1) / 2).clamp(0, 1)\n",
    "            gen_images = ((gen_images + 1) / 2).clamp(0, 1)\n",
    "\n",
    "            gen_images = torch.cat(\n",
    "                [gen_image for gen_image in gen_images], 2)\n",
    "\n",
    "            attr_images = torch.cat(\n",
    "                [attr_image for attr_image in attr_images], 2)\n",
    "\n",
    "            gen_results = torch.cat([id_images[0], gen_images], 2)\n",
    "\n",
    "            attr_images = torch.cat(\n",
    "                [torch.ones(3, 256, 256).to(device), attr_images], 2)\n",
    "            gen_results = torch.cat([attr_images, gen_results], 1)\n",
    "\n",
    "    return gen_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.name = \"exp_11\"\n",
    "args.weights_dir = args.results_dir.joinpath(args.name+\"/weights\")\n",
    "gen_results_0 = generate_image(args)\n",
    "# args.name = \"exp_05-no_concatenation\"\n",
    "# args.weights_dir = args.results_dir.joinpath(args.name+\"/weights\")\n",
    "# gen_results_1 = generate_image(args)\n",
    "# args.name = \"exp_06-add_style_regularizer\"\n",
    "# args.weights_dir = args.results_dir.joinpath(args.name+\"/weights\")\n",
    "# gen_results_2 = generate_image(args)\n",
    "# args.name = \"exp_11\"\n",
    "# args.weights_dir = args.results_dir.joinpath(args.name+\"/weights\")\n",
    "# gen_results_3 = generate_image(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF.to_pil_image(gen_results_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_results_0[0] / 1000, gen_results_0[1] / \\\n",
    "    1000, gen_results_0[2] / 1000 * 180 / 3.1415,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2 as cv\n",
    "from imutils import face_utils\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\n",
    "    \"./pretrained/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "while True:\n",
    "    # Getting out image by webcam\n",
    "    image = cv.imread(\"./dataset/ffhq256_dataset/image/01000/01000.png\")\n",
    "    # Converting the image to gray scale\n",
    "    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Get faces into webcam's image\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    # For each detected face, find the landmark.\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # Make the prediction and transfom it to numpy array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        # Draw on our image, all the finded cordinate points (x,y)\n",
    "        for (x, y) in shape:\n",
    "            cv.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "    # Show the image\n",
    "    cv.imshow(\"Output\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.name = \"exp_05-no_concatenation\"\n",
    "args.weights_dir = args.results_dir.joinpath(args.name+\"/weights\")\n",
    "gen_results_1 = generate_image(args, id_images, id_img_seed, attr_images)\n",
    "args.name = \"exp_06-add_style_regularizer\"\n",
    "args.weights_dir = args.results_dir.joinpath(args.name+\"/weights\")\n",
    "gen_results_2 = generate_image(args, id_images, id_img_seed, attr_images)\n",
    "args.name = \"exp_11\"\n",
    "args.weights_dir = args.results_dir.joinpath(args.name+\"/weights\")\n",
    "gen_results_3 = generate_image(args, id_images, id_img_seed, attr_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_results = torch.cat([gen_results_1[:, 256:],\n",
    "                        gen_results_2[:, 256:], gen_results_3[:, 256:]], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img01 = cv.imread(\"./output2.png\")\n",
    "img01 = torch.from_numpy(img01.transpose((2, 0, 1))).float()\n",
    "img01 = img01.flip(-3)\n",
    "\n",
    "img02 = cv.imread(\"./output1.png\")\n",
    "img02 = torch.from_numpy(img02.transpose((2, 0, 1))).float()\n",
    "img02 = img02.flip(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.cat([img01, img02], 1)\n",
    "img.sub_(127.5).div_(128)\n",
    "\n",
    "img = ((img + 1) / 2).clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF.to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_scale = 0\n",
    "pbar = tqdm(range(10000), ncols=80)\n",
    "for _ in pbar:\n",
    "    images = next(ffhq_iter)\n",
    "    images = images.to(device)\n",
    "\n",
    "    lnds_results = network.generator.landmarks_detector(images.flip(-3))\n",
    "\n",
    "    if lnds_results is None:\n",
    "        continue\n",
    "\n",
    "    lnds, poses = lnds_results[4], lnds_results[1]\n",
    "\n",
    "    face_scale += lnds[0, 0, 16] - lnds[0, 0, 0]\n",
    "\n",
    "mean_face_scale = face_scale / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_face_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_img_path = \"gen_dataset/image/8000/7029.png\"\n",
    "id_img_seed = int(os.path.splitext(os.path.basename(id_img_path))[0])\n",
    "\n",
    "attr_img_path = \"gen_dataset/image/1000/964.png\"\n",
    "\n",
    "id_image = cv.imread(str(args.dataset_path.joinpath(id_img_path)))\n",
    "attr_image = cv.imread(str(args.dataset_path.joinpath(attr_img_path)))\n",
    "\n",
    "id_image = torch.from_numpy(id_image.transpose(\n",
    "    (2, 0, 1))).float().to(DEVICE).flip(-3)\n",
    "id_image.sub_(127.5).div_(128)\n",
    "attr_image = torch.from_numpy(attr_image.transpose(\n",
    "    (2, 0, 1))).float().to(DEVICE).flip(-3)\n",
    "attr_image.sub_(127.5).div_(128)\n",
    "\n",
    "show_image = torch.concat((id_image, attr_image), -1)\n",
    "TF.to_pil_image(((show_image+1)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_lnd_results = network.generator.landmarks_detector(\n",
    "    attr_image.flip(-3)[None, ...])\n",
    "attr_calib_lnds = attr_lnd_results[2]\n",
    "\n",
    "id_lnd_results = network.generator.landmarks_detector(\n",
    "    id_image.flip(-3)[None, ...])\n",
    "id_calib_lnds = id_lnd_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "pil_attr = TF.to_pil_image((id_image+1)/2)\n",
    "\n",
    "image_draw = ImageDraw.Draw(pil_attr)\n",
    "# for x, y, _ in attr_calib_lnds[0, :, :3]:\n",
    "#     image_landmark_coords = [(x-1, y-1), (x+1, y+1)]\n",
    "#     image_draw.ellipse(image_landmark_coords, fill=\"red\")\n",
    "for x, y in id_lnd_results[2].permute(0, 2, 1)[0]:\n",
    "    image_landmark_coords = [(x-1, y-1), (x+1, y+1)]\n",
    "    image_draw.ellipse(image_landmark_coords, fill=\"blue\")\n",
    "# image_draw.line([tuple(ori_o.squeeze().tolist()[:2]), tuple(\n",
    "#     ori_e1.squeeze().tolist()[:2])], fill=\"green\", width=2)\n",
    "# image_draw.line([tuple(ori_o.squeeze().tolist()[:2]), tuple(\n",
    "#     ori_e2.squeeze().tolist()[:2])], fill=\"green\", width=2)\n",
    "# image_draw.line([tuple(ori_o.squeeze().tolist()[:2]), tuple(\n",
    "#     ori_e3.squeeze().tolist()[:2])], fill=\"green\", width=2)\n",
    "\n",
    "# image_draw.line([tuple(ori_o.squeeze().tolist()[:2]), tuple(\n",
    "#     new_e1.squeeze().tolist()[:2])], fill=\"red\", width=2)\n",
    "# image_draw.line([tuple(ori_o.squeeze().tolist()[:2]), tuple(\n",
    "#     new_e2.squeeze().tolist()[:2])], fill=\"red\", width=2)\n",
    "# image_draw.line([tuple(ori_o.squeeze().tolist()[:2]), tuple(\n",
    "#     new_e3.squeeze().tolist()[:2])], fill=\"red\", width=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_lnds = attr_lnds.permute(0, 2, 1)\n",
    "id_lnds = id_lnds.permute(0, 2, 1)\n",
    "\n",
    "attr_recov_lnds = attr_lnds.matmul(attr_R[0].T.inverse())\n",
    "id_recov_lnds = id_lnds.matmul(id_R[0].T.inverse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_o = (src_lnds[0, :, 0] + src_lnds.reshape(1, 68, 3)[0, 16, :]) / 2\n",
    "ori_e1 = ori_o\n",
    "ori_e2 = torch.tensor([128., 178., 0.], device=DEVICE)\n",
    "ori_e3 = torch.tensor([128., 128., 50.], device=DEVICE)\n",
    "new_o = ori_o[None, ...].mm(R[0].T)\n",
    "new_e1 = ori_e1[None, ...].mm(R[0].T)\n",
    "new_e2 = ori_e2[None, ...].mm(R[0].T)\n",
    "new_e3 = ori_e3[None, ...].mm(R[0].T)\n",
    "# new_e1[:, 1] = - new_e1[:, 1]\n",
    "# new_e2[:, 1] = - new_e2[:, 1]\n",
    "# new_e3[:, 1] = - new_e3[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_e1 = new_e1.mm(R[0].T.inverse())\n",
    "ori_e2 = new_e2.mm(R[0].T.inverse())\n",
    "ori_e3 = new_e3.mm(R[0].T.inverse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_padding = torch.zeros((1, 1344)).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    # z = torch.randn((1, 512)).to(DEVICE).clamp(-1, 1)\n",
    "    z = torch.from_numpy(np.random.RandomState(\n",
    "        id_img_seed).randn(1, 512)).to(DEVICE)\n",
    "    ws = network.generator.stylegan_generator.mapping(z, 0)\n",
    "    # ctrlv = torch.zeros((1, 6048)).to(DEVICE)\n",
    "\n",
    "    # gen_id_image = network.generator.stylegan_generator.synthesis(ws, ctrlv)\n",
    "\n",
    "    id_embedding = network.generator.id_encoder(id_image[None, ...])\n",
    "    attr_embedding = network.generator.attr_encoder(\n",
    "        torch.broadcast_to(attr_image, [1, *attr_image.shape]))\n",
    "\n",
    "    # feature_input = torch.concat([id_embedding, attr_embedding], -1)\n",
    "    feature_input = attr_embedding\n",
    "\n",
    "    if args.parameter_embedding:\n",
    "        pose_sp_embedding = network.generator.reference_pose_encoder(\n",
    "            feature_input)\n",
    "        pose_control_vector = network.generator.reference_pose_decoder(\n",
    "            pose_sp_embedding)\n",
    "        expression_sp_embedding = network.generator.reference_expression_encoder(\n",
    "            feature_input)\n",
    "        expression_control_vector = network.generator.reference_expression_decoder(\n",
    "            expression_sp_embedding)\n",
    "        base_control_vector = 0\n",
    "    else:\n",
    "        base_control_vector = network.generator.reference_network(\n",
    "            feature_input)\n",
    "        pose_control_vector = expression_control_vector = 0\n",
    "\n",
    "    control_vector = base_control_vector + \\\n",
    "        pose_control_vector + expression_control_vector\n",
    "\n",
    "    control_vector = torch.cat([control_vector, style_padding], -1)\n",
    "\n",
    "    gen_image = network.generator.stylegan_generator.synthesis(\n",
    "        ws, control_vector)\n",
    "\n",
    "TF.to_pil_image(make_grid(\n",
    "    [((id_image + 1) / 2).clamp(0, 1), ((gen_image[0] + 1) / 2).clamp(0, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_control_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = ((gen_image + 1) / 2 * 255).clamp(0, 255).to(torch.uint8)\n",
    "test_img = ((gen_id_image + 1) / 2 * 255).clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "train_img = TF.resize(train_img, (299, 299)).cpu()\n",
    "test_img = TF.resize(test_img, (299, 299)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.frechet_inception_distance import FIDScore\n",
    "\n",
    "fid_score = FIDScore()\n",
    "\n",
    "fid_score.calculate_fid(train_img, test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "fid = FrechetInceptionDistance()\n",
    "fid.update(test_img, real=True)\n",
    "fid.update(train_img, real=False)\n",
    "fid.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(network, id_image_path, attr_image_path_list, feature_embedding=False):\n",
    "\n",
    "    id_image = cv.imread(str(args.dataset_path.joinpath(id_image_path)))\n",
    "    id_image_seed = int(os.path.splitext(os.path.basename(id_image_path))[0])\n",
    "\n",
    "    id_image = torch.from_numpy(id_image.transpose(\n",
    "        (2, 0, 1))).float().to(DEVICE).flip(-3)\n",
    "    id_image.sub_(127.5).div_(128)\n",
    "    id_image = id_image.expand(len(attr_image_path_list), *id_image.shape)\n",
    "\n",
    "    attr_image_list = []\n",
    "    for attr_image_path in attr_image_path_list:\n",
    "\n",
    "        attr_image = cv.imread(\n",
    "            str(args.dataset_path.joinpath(attr_image_path)))\n",
    "        attr_image = torch.from_numpy(attr_image.transpose(\n",
    "            (2, 0, 1))).float().to(DEVICE).flip(-3)\n",
    "        attr_image.sub_(127.5).div_(128)\n",
    "\n",
    "        attr_image_list.append(attr_image[None, ...])\n",
    "\n",
    "    attr_image = torch.concat(attr_image_list, 0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # z = torch.randn((6,512)).to(DEVICE).clamp(-1, 1)\n",
    "        zs = torch.from_numpy(np.random.RandomState(id_image_seed).randn(\n",
    "            1, 512)).to(DEVICE).expand(len(attr_image_path_list), 512)\n",
    "        ws = network.generator.stylegan_generator.mapping(zs, 0)\n",
    "        # ctrlv = torch.zeros((1, 6048)).to(DEVICE)\n",
    "\n",
    "        # gen_id_image = network.generator.stylegan_generator.synthesis(ws, ctrlv)\n",
    "\n",
    "        id_embedding = network.generator.id_encoder(id_image)\n",
    "        attr_embedding = network.generator.attr_encoder(attr_image)\n",
    "\n",
    "        feature_input = torch.concat([id_embedding, attr_embedding], -1)\n",
    "\n",
    "        if feature_embedding:\n",
    "            pose_sp_embedding = network.generator.reference_pose_encoder(\n",
    "                feature_input)\n",
    "            pose_control_vector = network.generator.reference_pose_decoder(\n",
    "                pose_sp_embedding)\n",
    "            expression_sp_embedding = network.generator.reference_expression_encoder(\n",
    "                feature_input)\n",
    "            expression_control_vector = network.generator.reference_expression_decoder(\n",
    "                expression_sp_embedding)\n",
    "            base_control_vector = 0\n",
    "        else:\n",
    "            base_control_vector = network.generator.reference_network(\n",
    "                feature_input)\n",
    "            pose_control_vector = expression_control_vector = 0\n",
    "\n",
    "        control_vector = base_control_vector + \\\n",
    "            pose_control_vector + expression_control_vector\n",
    "\n",
    "        gen_image = network.generator.stylegan_generator.synthesis(\n",
    "            ws, control_vector)\n",
    "\n",
    "        out_image = torch.concat((attr_image, gen_image, id_image), -1)\n",
    "\n",
    "    return gen_image, out_image, control_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_image_path = \"image/4000/3000.png\"\n",
    "# id_image_path = \"image/1000/877.png\"\n",
    "attr_image_path_list = [\"image/1000/964.png\", \"image/1000/865.png\",\n",
    "                        \"image/1000/877.png\", \"image/1000/14.png\", \"image/1000/26.png\"]\n",
    "gen_image, out_image, control_vector = evaluation(\n",
    "    network, id_img_path, attr_image_path_list, args.feature_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF.to_pil_image(((out_image[0]+1)/2).clamp(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(control_vector[0].abs(), 10).values, torch.topk(\n",
    "    control_vector[0].abs(), 10).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(control_vector[1].abs(), 10).values, torch.topk(\n",
    "    control_vector[1].abs(), 10).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(control_vector[2].abs(), 10).values, torch.topk(\n",
    "    control_vector[2].abs(), 10).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(control_vector[3].abs(), 10).values, torch.topk(\n",
    "    control_vector[3].abs(), 10).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(control_vector[4].abs(), 10).values, torch.topk(\n",
    "    control_vector[4].abs(), 10).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "channels = list(range(4928))\n",
    "values = control_vector[0].tolist()\n",
    "\n",
    "ax.bar(channels, values)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "channels = list(range(4928))\n",
    "values = control_vector[1].squeeze().cpu().numpy()\n",
    "\n",
    "ax.bar(channels, values)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "fid = FrechetInceptionDistance(feature=2048)\n",
    "\n",
    "# real_image = cv.imread(\"./dataset/ffhq256_dataset/00000/00037.png\")\n",
    "# real_image = torch.from_numpy(real_image.transpose((2, 0, 1)))[None, ...]\n",
    "# real_image = real_image.flip(-3)\n",
    "# real_image = TF.resize(real_image, (299, 299))\n",
    "\n",
    "real_image = ((gen_id_image + 1) / 2 * 255).clamp(0, 255).to(torch.uint8).cpu()\n",
    "fake_image = TF.resize(real_image, (299, 299))\n",
    "fake_image = ((gen_image + 1) / 2 * 255).clamp(0, 255).to(torch.uint8).cpu()\n",
    "fake_image = TF.resize(fake_image, (299, 299))\n",
    "\n",
    "fid.update(real_image, real=True)\n",
    "fid.update(fake_image, real=False)\n",
    "fid.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(torch.nn.functional.mse_loss(gen_id_image, gen_image)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import peak_signal_noise_ratio as psnr_metric\n",
    "\n",
    "psnr_metric(id_image, gen_image, data_range=1.0).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
