{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attr_Encoder loads checkpoint from: output/exp_07/weights/Attr_Encoder.pth\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReferenceNetwork' object has no attribute '_load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspace/stylespace_plus/evaluation.ipynb Cell 1\u001b[0m in \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374796c6573706163655f706c7573227d/workspace/stylespace_plus/evaluation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m stylegan_G_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(args\u001b[39m.\u001b[39mpretrained_models_path\u001b[39m.\u001b[39mjoinpath(\u001b[39m\"\u001b[39m\u001b[39mffhq.pkl\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374796c6573706163655f706c7573227d/workspace/stylespace_plus/evaluation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m landmarks_model_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(args\u001b[39m.\u001b[39mpretrained_models_path\u001b[39m.\u001b[39mjoinpath(\u001b[39m'\u001b[39m\u001b[39m3DDFA/phase1_wpdc_vdc.pth.tar\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374796c6573706163655f706c7573227d/workspace/stylespace_plus/evaluation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m network \u001b[39m=\u001b[39m Network(args\u001b[39m=\u001b[39;49margs, id_model_path\u001b[39m=\u001b[39;49mid_model_path, base_generator_path\u001b[39m=\u001b[39;49mstylegan_G_path, \n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374796c6573706163655f706c7573227d/workspace/stylespace_plus/evaluation.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m                   landmarks_detector_path\u001b[39m=\u001b[39;49mlandmarks_model_path, device\u001b[39m=\u001b[39;49mDEVICE, load_chackpoint\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mload_checkpoint)\n",
      "File \u001b[0;32m/workspace/stylespace_plus/model/network.py:18\u001b[0m, in \u001b[0;36mNetwork.__init__\u001b[0;34m(self, args, id_model_path, base_generator_path, landmarks_detector_path, device, load_chackpoint)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator \u001b[39m=\u001b[39m Generator(args\u001b[39m=\u001b[39margs, \n\u001b[1;32m     12\u001b[0m                            id_model_path\u001b[39m=\u001b[39mid_model_path, \n\u001b[1;32m     13\u001b[0m                            base_generator_path\u001b[39m=\u001b[39mbase_generator_path, \n\u001b[1;32m     14\u001b[0m                            landmarks_detector_path\u001b[39m=\u001b[39mlandmarks_detector_path, \n\u001b[1;32m     15\u001b[0m                            device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m load_chackpoint:\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load()\n",
      "File \u001b[0;32m/workspace/stylespace_plus/model/network.py:33\u001b[0m, in \u001b[0;36mNetwork._load\u001b[0;34m(self, reason)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load\u001b[39m(\u001b[39mself\u001b[39m, reason\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerator\u001b[39m.\u001b[39;49m_load(reason)\n",
      "File \u001b[0;32m/workspace/stylespace_plus/model/generator.py:110\u001b[0m, in \u001b[0;36mGenerator._load\u001b[0;34m(self, reason)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load\u001b[39m(\u001b[39mself\u001b[39m, reason):\n\u001b[1;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattr_encoder\u001b[39m.\u001b[39m_load(reason)\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreference_network\u001b[39m.\u001b[39;49m_load(reason)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1260\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1259\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1260\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1261\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReferenceNetwork' object has no attribute '_load'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from model.network import Network\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.name = \"exp_07\"\n",
    "        self.resolution = 256\n",
    "        self.load_checkpoint = True\n",
    "        self.train = False\n",
    "        self.dataset_path = Path(\"./dataset/gen_dataset/\")\n",
    "        self.results_dir = Path(\"./output\")\n",
    "        self.pretrained_models_path = Path(\"./pretrained\")\n",
    "        self.train_data_size = 50000\n",
    "        self.batch_size = 6\n",
    "        self.reals = False\n",
    "        self.test_real_attr = True\n",
    "        self.train_real_attr = False\n",
    "        self.weights_dir = self.results_dir.joinpath(self.name+\"/weights\")\n",
    "\n",
    "args = Args()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "id_model_path = str(args.pretrained_models_path.joinpath(\"resnet50_scratch_weight.pkl\"))\n",
    "stylegan_G_path = str(args.pretrained_models_path.joinpath(\"ffhq.pkl\"))\n",
    "landmarks_model_path = str(args.pretrained_models_path.joinpath('3DDFA/phase1_wpdc_vdc.pth.tar'))\n",
    "\n",
    "network = Network(args=args, id_model_path=id_model_path, base_generator_path=stylegan_G_path, \n",
    "                  landmarks_detector_path=landmarks_model_path, device=DEVICE, load_chackpoint=args.load_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_image = cv.imread(str(args.dataset_path.joinpath(\"image/1000/3.png\")))\n",
    "attr_image = cv.imread(str(args.dataset_path.joinpath(\"image/1000/29.png\")))\n",
    "\n",
    "id_image = torch.from_numpy(id_image.transpose((2, 0, 1))).float().to(DEVICE)\n",
    "attr_image = torch.from_numpy(attr_image.transpose((2, 0, 1))).float().to(DEVICE)\n",
    "\n",
    "TF.to_pil_image(make_grid([id_image.flip(-3).to(torch.uint8), attr_image.flip(-3).to(torch.uint8)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_image.sub_(127.5).div_(128)\n",
    "attr_image.sub_(127.5).div_(128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    #z = torch.randn((6,512)).to(DEVICE).clamp(-1, 1)\n",
    "    z = torch.from_numpy(np.load(str(args.dataset_path.joinpath(\"z/1000/3.npy\")))).to(DEVICE)\n",
    "    ws = network.generator.stylegan_generator.mapping(z, 0)\n",
    "    ctrlv = torch.zeros((6, 6048)).to(DEVICE)\n",
    "    \n",
    "    gen_id_image = network.generator.stylegan_generator.synthesis(ws, ctrlv)\n",
    "    gen_id_image = TF.resize(gen_id_image, (256, 256))\n",
    "    \n",
    "    id_embedding = network.generator.id_encoder(gen_id_image)\n",
    "    attr_embedding = network.generator.attr_encoder(torch.broadcast_to(attr_image, [6, *attr_image.shape]))\n",
    "    \n",
    "    feature_tag = torch.concat([id_embedding, attr_embedding], -1)\n",
    "    \n",
    "    pose_sp_embedding = network.generator.reference_pose_encoder(feature_tag)\n",
    "    pose_control_vector = network.generator.reference_pose_decoder(pose_sp_embedding)\n",
    "    expression_sp_embedding = network.generator.reference_expression_encoder(feature_tag)\n",
    "    expression_control_vector = network.generator.reference_expression_decoder(expression_sp_embedding)\n",
    "\n",
    "    control_vector = pose_control_vector + expression_control_vector\n",
    "    \n",
    "    gen_image = network.generator.stylegan_generator.synthesis(ws, control_vector)\n",
    "    gen_image = TF.resize(gen_image, (256, 256))\n",
    "\n",
    "TF.to_pil_image(make_grid([((gen_id_image[0] + 1) / 2).clamp(0, 1), ((gen_image[0] + 1) / 2).clamp(0, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "fid = FrechetInceptionDistance(feature=2048)\n",
    "\n",
    "# real_image = cv.imread(\"./dataset/ffhq256_dataset/00000/00037.png\")\n",
    "# real_image = torch.from_numpy(real_image.transpose((2, 0, 1)))[None, ...]\n",
    "# real_image = real_image.flip(-3)\n",
    "# real_image = TF.resize(real_image, (299, 299))\n",
    "\n",
    "real_image = ((gen_id_image + 1) / 2 * 255).clamp(0, 255).to(torch.uint8).cpu()\n",
    "fake_image = TF.resize(real_image, (299, 299))\n",
    "fake_image = ((gen_image + 1) / 2 * 255).clamp(0, 255).to(torch.uint8).cpu()\n",
    "fake_image = TF.resize(fake_image, (299, 299))\n",
    "\n",
    "fid.update(real_image, real=True)\n",
    "fid.update(fake_image, real=False)\n",
    "fid.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(torch.nn.functional.mse_loss(gen_id_image, gen_image)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import peak_signal_noise_ratio as psnr_metric\n",
    "\n",
    "psnr_metric(gen_image, gen_id_image, data_range=1.0).item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
