{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"A6YWPu3n06m4"},"outputs":[],"source":["# Hyper parameters - default\n","USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n","\n","# Hyper parameters - adjustable\n","BATCH_SIZE = 10\n","MAX_EPOCH = 1000\n","LR_RATE = 0.001\n","Z_DIM = 512\n","OUTPUT_DIR = \"output\"\n","NETWORK_PKL = \"pretrained/ffhq.pkl\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8JUASaI706m4"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" to /home/rodney/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2641d9afd0b74d47ae2ef982ac6ee4b7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/85.7M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Downloading: \"https://www.adrianbulat.com/downloads/python-fan/2DFAN4-cd938726ad.zip\" to /home/rodney/.cache/torch/hub/checkpoints/2DFAN4-cd938726ad.zip\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44531e18834240d99a33a1f490f444e1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/91.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# DataLoader\n","train_dataset = FaceLandmarksDataset(\"train\", Transforms())\n","train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n","\n","validate_dataset = FaceLandmarksDataset(\"validate\", Transforms())\n","validate_loader = DataLoader(validate_dataset, BATCH_SIZE, shuffle=True)\n","\n","# Network\n","with dnnlib.util.open_url(NETWORK_PKL) as f:\n","    data = legacy.load_network_pkl(f)\n","    generator = data[\"G_ema\"].to(DEVICE)\n","    discriminator = data[\"D\"].to(DEVICE)\n","\n","reference_encoder = ReferenceEncoder()\n","fa_network = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False)\n","\n","# Loss function & optimizer\n","mse_loss  = nn.MSELoss()\n","optimizer = optim.Adam(reference_encoder.parameters(), lr=LR_RATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"en6z7cXb06m4"},"outputs":[],"source":["# Print loading information\n","print('Loading networks from \"%s\"...' % NETWORK_PKL)\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# Log preparation\n","config = dict(\n","    epochs = MAX_EPOCH,\n","    batch_size = BATCH_SIZE,\n","    lr_rate = LR_RATE \n",")\n","run = wandb.init(project = \"style_plus\", config = config)\n","wandb.watch(generator, log_freq=100)\n","\n","# Training process\n","loss_min = np.inf\n","for epoch in range(1, MAX_EPOCH+1):\n","\n","    loss_train = 0\n","    loss_validate = 0\n","    running_loss = 0\n","\n","    reference_encoder.train()\n","    for step in range(1, len(train_loader)+1):\n","        images, landmarks = next(iter(train_loader))\n","\n","        images = images.to(DEVICE)\n","        landmarks = landmarks.view(landmarks.shape[0], -1).to(DEVICE)\n","    \n","        # Reference image encoding\n","        ref_code = reference_encoder(images)\n","        \n","        # StyleGANs image synthesis\n","        # W space encoding\n","        seed = random.randint(0, 2**23-1)\n","        z = torch.tensor(np.random.RandomState(seed).randn(BATCH_SIZE, generator.z_dim), requires_grad=True).to(DEVICE)\n","        ws = generator.mapping(z, 0)\n","\n","        # W, Style+ code mixing\n","        generated_image = generator.synthesis(ws + ref_code.view(BATCH_SIZE, 18, 512))\n","        generated_image = (generated_image * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","        \n","        # Facial landmark detection\n","        pred_landmarks = fa_network.get_landmarks(generated_image[0].permute(1, 2, 0))\n","        pred_landmarks = torch.tensor(pred_landmarks[0], requires_grad=True).to(DEVICE)\n","        pred_landmarks = pred_landmarks[None, :]\n","            \n","        for i in range(1, generated_image.shape[0]):\n","            _temp = fa_network.get_landmarks(generated_image[i].permute(1, 2, 0))\n","            _temp = torch.tensor(_temp[0]).to(DEVICE)\n","            pred_landmarks = torch.cat((pred_landmarks, _temp[None, :]), 0)\n","        \n","        # find the loss for the current step\n","        loss_train_step = mse_loss(landmarks, pred_landmarks.view(pred_landmarks.shape[0], -1))\n","\n","        # calculate the gradients\n","        loss_train_step.backward()\n","\n","        # update the parameters\n","        optimizer.step()\n","\n","        loss_train += loss_train_step.item()\n","        running_loss = loss_train/step\n","\n","        # Log\n","        print(f\"traing process - loss_train: {loss_train:.4f}; running_loss: {running_loss:.4f}\")\n","\n","\n","    reference_encoder.eval()\n","    with torch.no_grad():\n","        for step in range(1, len(validate_loader)+1):\n","            images, landmarks = next(iter(validate_loader))\n","\n","            images = images.to(DEVICE)\n","            landmarks = landmarks.view(landmarks.shape[0], -1).to(DEVICE)\n","    \n","            # Reference image encoding\n","            ref_code = reference_encoder(images)\n","\n","            # StyleGANs image synthesis\n","            # W space encoding\n","            seed = random.randint(0, 2**23-1)\n","            z = torch.from_numpy(np.random.RandomState(seed).randn(BATCH_SIZE, generator.z_dim)).to(DEVICE)\n","            ws = generator.mapping(z, 0)\n","\n","            # W, Style+ code mixing\n","            generated_image = generator.synthesis(ws + ref_code.view(BATCH_SIZE, 18, 512))\n","            generated_image = (generated_image.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","        \n","            # Facial landmark detection\n","            pred_landmarks = fa_network.get_landmarks(generated_image[0].permute(1, 2, 0))\n","            pred_landmarks = torch.tensor(pred_landmarks[0]).to(DEVICE)\n","            pred_landmarks = pred_landmarks[None, :]\n","            \n","            for i in range(1, generated_image.shape[0]):\n","                _temp = fa_network.get_landmarks(generated_image[i].permute(1, 2, 0))\n","                _temp = torch.tensor(_temp[0]).to(DEVICE)\n","                pred_landmarks = torch.cat((pred_landmarks, _temp[None, :]), 0)\n","        \n","            # find the loss for the current step\n","            loss_validate_step = mse_loss(landmarks, pred_landmarks)\n","\n","            loss_validate += loss_validate_step.item()\n","            running_loss = loss_validate/step\n","\n","            # Log\n","            print(f\"validating process - loss_validate: {loss_validate:.4f}; running_loss: {running_loss:.4f}\")\n","     \n","    loss_train /= len(train_loader)\n","    loss_validate /= len(validate_loader)\n","\n","    # Log\n","    wandb.log(\"loss_train\", loss_train)\n","    wandb.log(\"loss_validate\", loss_validate)\n","\n","    if loss_validate < loss_min:\n","        loss_min = loss_validate\n","        torch.save(reference_encoder.state_dict(), OUTPUT_DIR + \"/saved_model/reference_encoder.pth\")\n","        PIL.Image.fromarray(generated_image[0].permute(1, 2, 0).cpu().numpy(), \"RGB\").save(f\"{OUTPUT_DIR}/{seed:04d}_{epoch}/seed{seed:04d}_gen.png\")\n","        PIL.Image.fromarray(images[0].cpu().numpy(), \"RGB\").save(f'{OUTPUT_DIR}/{seed:04d}_{epoch}/seed{seed:04d}_ref.png')\n","        print(\"\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\".format(loss_min, epoch, MAX_EPOCH))\n","        print(\"Model Saved\\n\")"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"train.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"4c66935637d36ba66a5aee72b862396307fbd041b6e21a3712960dc2e5d2386e"}}},"nbformat":4,"nbformat_minor":0}
