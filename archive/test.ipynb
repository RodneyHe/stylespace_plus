{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import random, json\n","from torch.utils.data import DataLoader\n","import torch\n","import torchvision.transforms.functional as TF\n","from torchvision import models\n","import torch.optim as optim\n","import dnnlib, legacy\n","from data.GEN_data import FaceLandmarksDataset, Transforms\n","from training_modified import networks\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n","\n","train_dataset = FaceLandmarksDataset(\"train\", transform=Transforms())\n","train_loader = DataLoader(train_dataset, 1, shuffle=True)\n","\n","with dnnlib.util.open_url(\"pretrained/ffhq.pkl\") as f:\n","    data = legacy.load_network_pkl(f)\n","    generator = data[\"G_ema\"].to(DEVICE)\n","    discriminator = data[\"D\"].to(DEVICE)\n","\n","style_generator = networks.Generator(**generator.init_kwargs).to(DEVICE)\n","style_generator.load_state_dict(generator.state_dict())\n","style_generator.eval()\n","\n","ran_z = torch.randn((1, 512)).to(DEVICE)\n","spv = torch.zeros((1, 6048)).to(DEVICE)\n","\n","ref_ws = style_generator.mapping(ran_z, 0)\n","\n","gen_images = style_generator.synthesis(ref_ws, spv)\n","gen_images_denorm = (gen_images * 127.5 + 128).clamp(0, 255) / 255"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TF.to_pil_image(TF.resize(gen_images_denorm[0], (256, 256)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from PIL import Image, ImageDraw\n","\n","\n","draw = ImageDraw.Draw(ref_img)\n","\n","r = 4\n","\n","for x, y in ref_landmarks[0].tolist():\n","    leftUpPoint = (x-r, y-r)\n","    rightDownPoint = (x+r, y+r)\n","    twoPointList = [leftUpPoint, rightDownPoint]\n","    draw.ellipse(twoPointList, fill=\"red\")\n","\n","ref_img.save(\"test.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_images(ref_images = ref_images, gen_images=gen_images_denorm, size=(256,256), output_dir=\"./test.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["res = 1024\n","\n","feature_extractor = model.FeatureExtractor(style_generator.synthesis, [f\"b{res}.torgb\"])\n","\n","z = torch.tensor(json_data[\"images\"][str(23)][\"z\"]).to(DEVICE)\n","ws = style_generator.mapping(z, 0)\n","\n","spv_1 = torch.zeros((1, 6048), device=DEVICE)\n","\n","gen_image_1 = feature_extractor(ws, spv_1)[f\"b{res}.torgb\"][0]\n","gen_image_de_1 = (gen_image_1.permute(1,2,0) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","gen_image_2 = style_generator.synthesis(ws, spv_1)\n","gen_image_de_2 = (gen_image_2.permute(0,2,3,1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Image.fromarray(gen_image_de_1.cpu().numpy(), 'RGB')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Image.fromarray(gen_image_de_2[0].cpu().numpy(), 'RGB')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a = TF.resize(TF.to_tensor(Image.open(json_data[\"images\"][\"31\"][\"dir\"])), (256, 256))\n","b = TF.resize(TF.to_tensor(Image.open(json_data[\"images\"][\"23\"][\"dir\"])), (256, 256))\n","\n","a_pil = TF.to_pil_image(a)\n","b_pil = TF.to_pil_image(b)\n","\n","out_img = Image.new(\"RGB\", (512, 256))\n","out_img.paste(a_pil, (0, 0))\n","out_img.paste(b_pil, (256, 0))\n","out_img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def show_fun(idx):\n","    style = torch.tensor([j for i in json_data[\"images\"][str(idx)][\"styles\"] for j in i])\n","    style_diff = style - torch.tensor(json_data[\"style_train_mean\"])\n","    fig, ax = plt.subplots()\n","    ax.plot(style_diff)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["show_fun(23), show_fun(31), show_fun(34)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["style_mean = torch.tensor(json_data[\"style_train_mean\"]).to(DEVICE)\n","\n","# ref_images, ref_landmarks, ref_zs, ref_bboxs = next(iter(train_loader))\n","# ref_images = ref_images.to(DEVICE)\n","# ref_landmarks = ref_landmarks.to(DEVICE)\n","# ref_zs = ref_zs.to(DEVICE)\n","# ref_bboxs = ref_bboxs.to(DEVICE)\n","\n","ref_zs = torch.tensor(json_data[\"images\"][\"9\"][\"z\"]).to(DEVICE)\n","ref_lnd = torch.tensor(json_data[\"images\"][\"9\"][\"landmark\"]).to(DEVICE).unsqueeze(0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spv_1 = torch.zeros((1, 6048)).to(DEVICE)\n","\n","spw = ref_mapping_network(ref_lnd.reshape(ref_lnd.shape[0], -1))\n","spv_2 = spw * style_mean\n","\n","ws = style_generator.mapping(ref_zs, 0)\n","gen_images_1 = style_generator.synthesis(ws, spv_1)\n","gen_images_denorm_1 = (gen_images_1 * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","gen_images_2 = style_generator.synthesis(ws, spv_2)\n","gen_images_denorm_2 = (gen_images_2 * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","gen_img_1 = TF.to_pil_image(TF.resize(gen_images_denorm_1[0], (256, 256)))\n","gen_img_2 = TF.to_pil_image(TF.resize(gen_images_denorm_2[0], (256, 256)))\n","\n","out_img = Image.new(\"RGB\", (512, 256))\n","out_img.paste(gen_img_1, (0, 0))\n","out_img.paste(gen_img_2, (256, 0))\n","out_img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_fun(chn, start, stop, step):\n","    assert abs(stop-start) % step == 0\n","    num = int(abs(stop-start) / step)\n","    out_img = Image.new(\"RGB\", (num*256, 256))\n","    coun=0\n","    for i in range(start, stop, step):\n","\n","        spv_ = torch.zeros((1, 6048), device=DEVICE)\n","        spv_[:, chn] = i\n","\n","        ws = style_generator.mapping(ref_zs, 0)\n","        gen_images_ = style_generator.synthesis(ws, spv_)\n","        gen_images_denorm_ = (gen_images_ * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","        gen_img = TF.to_pil_image(TF.resize(gen_images_denorm_[0], (256, 256)))\n","\n","        out_img.paste(gen_img, (coun*256, 0))\n","        coun += 1\n","    return out_img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spv_1 = torch.zeros((1, 6048), device=DEVICE)\n","\n","spv_2 = torch.zeros((1, 6048), device=DEVICE)\n","spv_2[:, 4938] = 6\n","\n","spv_2 = spv_2 * style_mean\n","\n","ws = style_generator.mapping(ref_zs, 0)\n","\n","gen_images_1 = style_generator.synthesis(ws, spv_1)\n","gen_images_denorm_1 = (gen_images_1 * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","gen_images_2 = style_generator.synthesis(ws, spv_2)\n","gen_images_denorm_2 = (gen_images_2 * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","gen_img_1 = TF.to_pil_image(TF.resize(gen_images_denorm_1[0], (256, 256)))\n","gen_img_2 = TF.to_pil_image(TF.resize(gen_images_denorm_2[0], (256, 256)))\n","\n","out_img = Image.new(\"RGB\", (512, 256))\n","out_img.paste(gen_img_1, (0, 0))\n","out_img.paste(gen_img_2, (256, 0))\n","out_img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(38, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(938, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(1938, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(2938, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(3938, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(4938, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(5938, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(6000, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(4937, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(4936, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(4935, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(4934, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(4933, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(4932, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = test_fun(4931, -400, 400, 80)\n","out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(\"./dataset/gen_dataset/label.json\", \"r\") as jsonf:\n","    json_data = json.load(jsonf)\n","\n","ref_res = [512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 256, 256, 128, 128, 64, 64, 32]\n","\n","json_data[\"train_dataset_num\"] = 3000\n","json_data[\"validate_dataset_num\"] = 1000\n","\n","spv = []\n","for res in ref_res:\n","    w = torch.ones((1, int(res)))\n","    spv.append(w)\n","spv = torch.cat(spv, 1).to(DEVICE)\n","\n","style_train = []\n","style_validate = []\n","for idx in range(len(json_data[\"images\"])):\n","\n","    z = torch.tensor(json_data[\"images\"][str(idx)][\"z\"]).to(DEVICE)\n","    ws = style_generator.mapping(z, 0)\n","\n","    gen_images = style_generator.synthesis(ws, spv)\n","    gen_images = (gen_images * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","    original_style = []\n","    for block in style_generator.synthesis.children():\n","        for layer in block.children():\n","            if not isinstance(layer, networks.ToRGBLayer):\n","                original_style.append(layer.original_style.squeeze().tolist())\n","    \n","    json_data[\"images\"][str(idx)][\"styles\"] = original_style\n","    \n","    if json_data[\"images\"][str(idx)][\"type\"] == \"train\":\n","        style_train.append(original_style)\n","    else:\n","        style_validate.append(original_style)\n","\n","style_train_mean = []\n","style_validate_mean = []\n","for layer in range(len(ref_res)):\n","    for channel in range(ref_res[layer]):\n","        _acc = 0\n","        for idx in range(len(style_train)):\n","            _acc += style_train[idx][layer][channel]\n","        style_train_mean.append(_acc/len(style_train))\n","\n","        _acc = 0\n","        for idx in range(len(style_validate)):\n","            _acc += style_train[idx][layer][channel]\n","        style_validate_mean.append(_acc/len(style_validate))\n","\n","json_data[\"style_train_mean\"] = style_train_mean\n","json_data[\"style_validate_mean\"] = style_train_mean\n","\n","with open(\"./dataset/gen_dataset/label1.json\", \"w\") as jsonf:\n","    json.dump(json_data, jsonf)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["z = torch.tensor(json_data[\"images\"][str(0)][\"z\"]).to(DEVICE)\n","ws = style_generator.mapping(z, 0)\n","\n","ref_res = [512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 256, 256, 128, 128, 64, 64, 32]\n","\n","spv = []\n","for res in ref_res:\n","    w = torch.ones((1, int(res)))\n","    spv.append(w)\n","\n","spv = torch.cat(spv, 1).to(DEVICE)\n","\n","\n","gen_images = style_generator.synthesis(ws, spv)\n","gen_images = (gen_images * 127.5 + 128).clamp(0, 255)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["original_style = []\n","modified_style = []\n","for block in style_generator.synthesis.children():\n","    for layer in block.children():\n","        if not isinstance(layer, networks.ToRGBLayer):\n","            original_style.append(layer.original_style)\n","            modified_style.append(layer.modified_style)\n","\n","original_style = torch.cat(original_style, dim=1)\n","modified_style = torch.cat(modified_style, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import model\n","face_lnd_estimator = model.FaceLandmarkEstimator()\n","\n","with dnnlib.util.open_url(\"pretrained/ffhq.pkl\") as f:\n","    data = legacy.load_network_pkl(f)\n","    generator = data[\"G_ema\"].to(DEVICE)\n","    discriminator = data[\"D\"].to(DEVICE)\n","\n","style_generator = networks.Generator(**generator.init_kwargs).to(DEVICE)\n","fa_network = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False)\n","style_generator.load_state_dict(generator.state_dict())\n","style_generator.eval()\n","\n","ref_mapping_network = model.RefMappingNetwork().to(DEVICE)\n","ref_mapping_network.load_state_dict(torch.load(\"./output/saved_model/sp/mapping_network.pth\"))\n","ref_mapping_network.eval()\n","\n","inception_v3 = models.inception_v3(pretrained=True).to(DEVICE)\n","inception_v3.load_state_dict(torch.load(\"./output/saved_model/sp/inception_v3.pth\"))\n","inception_v3.eval()\n","inception_features = model.FeatureExtractor(inception_v3, [\"fc\"])\n","\n","ref_images, ref_landmarks = next(iter(training_loader))\n","ref_images = ref_images.to(DEVICE)\n","ref_landmarks = ref_landmarks.view(ref_landmarks.shape[0], -1).to(DEVICE)\n","\n","# seed = random.randint(0, 2**23 - 1)\n","# z = torch.tensor(np.random.RandomState(seed).randn(6, 512)).to(DEVICE)\n","# ws = style_generator.mapping(z, 0)\n","\n","ref_features = inception_features(ref_images)\n","spv = ref_mapping_network(ref_features[\"fc\"][0])\n","\n","# gen_images = style_generator.synthesis(ws, spv)\n","# gen_images = (gen_images * 127.5 + 128).clamp(0, 255)\n","\n","# def show_landmarks(image, landmarks, bbox=None, retuire_bbox=False):\n","#     fig, ax = plt.subplots()\n","#     ax.imshow(image)\n","#     ax.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n","#     if retuire_bbox:\n","#         bbox = bbox[0]\n","#         rect = patches.Rectangle((bbox[0], bbox[3] - (bbox[3] - bbox[1])), bbox[2]- bbox[0], bbox[3] - bbox[1], linewidth=1, edgecolor='g', facecolor='none')\n","#         ax.add_patch(rect)\n","#     plt.pause(0.001)\n","\n","# for i in range(6):\n","#     lnd = fa_network.get_landmarks_from_image(gen_images[i].permute(1,2,0))\n","#     if lnd != None:\n","#         show_landmarks(gen_images[i].detach().cpu().permute(1,2,0), lnd[0])\n","\n","# pred_landmarks, unusual_index = face_lnd_estimator(gen_images)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ref_res = [512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 256, 256, 128, 128, 64, 64, 32]\n","a = 0\n","b = 1024\n","for i in range(len(ref_res)):\n","    #print(a, b, ref_res[i])\n","    print(\"weight:\", spv[0][a:b].tolist()[0:int(ref_res[i]/2)])\n","    print(\"bias:\", spv[0][a:b].tolist()[int(ref_res[i]/2):ref_res[i]])\n","    a += ref_res[i]\n","    b += ref_res[i]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with dnnlib.util.open_url(\"pretrained/ffhq.pkl\") as f:\n","    generator = data[\"G_ema\"].to(DEVICE)\n","    data = legacy.load_network_pkl(f)\n","    discriminator = data[\"D\"].to(DEVICE)\n","style_generator = networks.Generator(**generator.init_kwargs).to(DEVICE)\n","fa_network = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False)\n","style_generator.load_state_dict(generator.state_dict())\n","style_generator.eval()\n","seed = random.randint(0, 2**23 - 1)\n","z = torch.tensor(np.random.RandomState(seed).randn(1, 512)).to(DEVICE)\n","ws = style_generator.mapping(z, 0)\n","\n","ref_res = [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 512, 512, 256, 256, 128, 128, 64]\n","\n","spv = []\n","for res in ref_res:\n","    w = torch.ones((1, int(res / 2)))\n","    b = torch.zeros((1, int(res / 2)))\n","    spv.append(torch.cat((w, b), 1))\n","\n","spv = torch.cat(spv, 1).to(DEVICE)\n","\n","gen_images = style_generator.synthesis(ws, spv)\n","gen_images = (gen_images * 127.5 + 128).clamp(0, 255)\n","\n","pred_ = fa_network.get_landmarks_from_image(gen_images.squeeze().permute(1, 2, 0))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Network\n","with dnnlib.util.open_url(\"pretrained/ffhq.pkl\") as f:\n","    data = legacy.load_network_pkl(f)\n","    generator = data[\"G_ema\"].to(DEVICE)\n","    discriminator = data[\"D\"].to(DEVICE)\n","\n","style_generator = networks.Generator(**generator.init_kwargs).to(DEVICE)\n","style_space_discriminator = model.StyleSpaceDiscriminator().to(DEVICE)\n","style_discriminator = networks.Discriminator(**discriminator.init_kwargs).to(DEVICE)\n","\n","face_lnd_estimator = model.FaceLandmarkEstimator()\n","\n","style_generator.load_state_dict(generator.state_dict())\n","style_generator.eval()\n","\n","style_discriminator.load_state_dict(discriminator.state_dict())\n","style_discriminator.eval()\n","\n","ref_mapping_network = model.RefMappingNetwork().to(DEVICE)\n","ref_mapping_network.load_state_dict(torch.load(\"./output/saved_model/sp\"+\"/mapping_network.pth\"))\n","ref_mapping_network.eval()\n","\n","inception_v3 = models.inception_v3(init_weights=True).to(DEVICE)\n","inception_v3.load_state_dict(torch.load(\"./output/saved_model/sp\"+\"/inception_v3.pth\"))\n","inception_v3.eval()\n","inception_features = model.FeatureExtractor(inception_v3, [\"fc\"])\n","\n","inception_v3_optimizer = optim.Adam(inception_v3.parameters(), lr=0.01)\n","mapping_network_optimizer = optim.Adam(ref_mapping_network.parameters(), lr=0.01)\n","style_discriminator_optimizer = optim.Adam(style_discriminator.parameters(), lr=0.01)\n","\n","# ref_res = [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 512, 512, 256, 256, 128, 128, 64]\n","\n","# spv = []\n","# for res in ref_res:\n","#     w = torch.ones((6, int(res / 2)))\n","#     b = torch.zeros((6, int(res / 2)))\n","#     spv.append(torch.cat((w, b), 1))\n","\n","# spv = torch.cat(spv, 1).to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["images, landmarks = next(iter(training_loader))\n","images = images.to(DEVICE)\n","landmarks = landmarks.view(landmarks.shape[0], -1).to(DEVICE)\n","\n","seed = random.randint(0, 2**23 - 1)\n","z = torch.tensor(np.random.RandomState(seed).randn(1, 512)).to(DEVICE)\n","ws = style_generator.mapping(z, 0)\n","\n","features = inception_features(images)\n","spv = ref_mapping_network(features[\"fc\"][0])\n","\n","generated_images = style_generator.synthesis(ws, spv)\n","generated_images = (generated_images * 127.5 + 128).clamp(0, 255)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ws.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["f_logit = style_discriminator(generated_images, 0)\n","r_logit = style_discriminator(TF.resize(images, (1024, 1024)), 0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["d_loss = torch.nn.functional.softplus(-r_logit) + torch.nn.functional.softplus(r_logit)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["d_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from os import listdir\n","from os.path import isfile, join\n","\n","onlyfiles = [f for f in listdir(\"./output/saved_img/lnd/\") if isfile(join(\"./output/saved_img/lnd/\", f))]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["logits_list = []\n","for image_path in onlyfiles:\n","    _image = Image.open(join(\"./output/saved_img/lnd/\", image_path))\n","    gen_image = TF.to_tensor(_image)[:,:,256:].to(DEVICE)\n","    ref_image = TF.to_tensor(_image)[:,:,0:256].to(DEVICE)\n","    f_logits = style_discriminator(TF.resize(gen_image[None, ...], (1024, 1024)), 0)\n","    r_logits = style_discriminator(TF.resize(ref_image[None, ...], (1024, 1024)), 0)\n","    print(f\"Image:{image_path}\", (torch.nn.functional.softplus(-r_logits)+torch.nn.functional.softplus(f_logits)).item())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["b = TF.to_tensor(a)[:,:,256:].to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["logits = style_discriminator(TF.resize(b[None, ...], (1024, 1024)), 0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.nn.functional.softplus(logits)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.nn.functional.softplus(f_logit), torch.nn.functional.softplus(r_logit)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TF.to_pil_image(generated_images[0].detach().cpu().to(torch.uint8))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["importlib.reload(sys.modules[\"model\"])\n","face_lnd_estimator = model.FaceLandmarkEstimator()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred, ind = face_lnd_estimator(generated_images)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def show_landmarks(image, landmarks, bbox=None, retuire_bbox=False):\n","    fig, ax = plt.subplots()\n","    ax.imshow(image)\n","    ax.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n","    if retuire_bbox:\n","        bbox = bbox[0]\n","        rect = patches.Rectangle((bbox[0], bbox[3] - (bbox[3] - bbox[1])), bbox[2]- bbox[0], bbox[3] - bbox[1], linewidth=1, edgecolor='g', facecolor='none')\n","        ax.add_patch(rect)\n","    plt.pause(0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generated_images.shape, pred.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["show_landmarks(generated_images[0].permute(1,2,0).detach().cpu().to(torch.uint8), pred[0].detach().cpu())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys, torch\n","import importlib\n","import face_alignment\n","import face_alignment.utils as fan_utils\n","import model\n","from ffhq_data import FaceLandmarksDataset, Transforms\n","from torch.utils.data import DataLoader\n","import torchvision.transforms.functional as TF\n","import matplotlib.pyplot as plt\n","\n","importlib.reload(sys.modules[\"model\"])\n","importlib.reload(sys.modules[\"face_alignment.utils\"])\n","import model\n","import face_alignment.utils as fan_utils\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n","\n","face_lnd_estimator = model.FaceLandmarkEstimator()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_dataset = FaceLandmarksDataset(\"training\", scope=30000, transform=Transforms())\n","training_loader = DataLoader(training_dataset, 6, shuffle=True)\n","\n","images, landmarks = next(iter(training_loader))\n","images = images.to(DEVICE)\n","landmarks = landmarks.view(landmarks.shape[0], -1).to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["i = 5\n","show_landmarks(images[i].permute(1,2,0).cpu(), l[i].cpu().detach())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["image = (images * 127.5 + 128).clamp(0, 255)\n","d = face_lnd_estimator.fa_network.face_detector.detect_from_image(image[0].permute(1,2,0).detach().cpu())\n","bbox_width = d[0][2] - d[0][0]\n","bbox_height = d[0][3] - d[0][1]\n","top = d[0][1]\n","left = d[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a = TF.crop(image[0].to(torch.uint8), int(top), int(left), int(bbox_height), int(bbox_width))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TF.to_pil_image(a)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["center = torch.tensor([d[0][2] - (d[0][2] - d[0][0]) / 2.0, d[0][3] - (d[0][3] - d[0][1]) / 2.0])\n","center[1] = center[1] - (d[0][3] - d[0][1]) * 0.12\n","scale = torch.tensor((d[0][2] - d[0][0] + d[0][3] - d[0][1]) / face_lnd_estimator.fa_network.face_detector.reference_scale)\n","\n","inp = face_lnd_estimator.differentiableCrop(image[0].permute(1,2,0), center, scale)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inp.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TF.to_pil_image(inp.permute(2,0,1).to(torch.uint8))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_ = face_lnd_estimator(generated_images)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.eye(3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["i = 4\n","show_landmarks(generated_images[5].to(torch.uint8).detach().cpu().permute(1,2,0), pred_[4].detach().cpu())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["landmarks_batch_loss = torch.pow((landmarks - pred_.view(6, -1)), 2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["landmark_batch_loss = 0\n","for i in range(6):\n","    pred_ = fa_network.get_landmarks_from_image(generated_images[i].permute(1, 2, 0))\n","\n","    if pred_ is not None and len(pred_[0]) == 68:\n","        pred_landmark = torch.from_numpy(pred_[0]).requires_grad_()\n","        landmark_batch_loss += torch.pow((landmarks[i] - pred_landmark.view(-1).to(DEVICE)), 2)\n","\n","if isinstance(landmark_batch_loss, torch.Tensor):\n","    landmark_training_loss = landmark_batch_loss.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list(ref_mapping_network.parameters())[0].grad"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["generated_images = style_generator.synthesis(ws, spv).detach()\n","generated_images = (generated_images * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","\n","original_style = []\n","modified_style = []\n","for block in style_generator.synthesis.children():\n","    for layer in block.children():\n","        if not isinstance(layer, networks.ToRGBLayer):\n","            original_style.append(layer.original_style)\n","            modified_style.append(layer.modified_style)\n","        \n","original_style = torch.cat(original_style, dim=1)\n","modified_style = torch.cat(modified_style, dim=1)\n","\n","original_style_score = style_space_discriminator(original_style)\n","modified_style_score = style_space_discriminator(modified_style)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Facial landmark detection\n","landmark_acc_loss = 0\n","bbox_acc_loss = 0\n","\n","a = (images * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n","for i in range(6):\n","    pred_ = fa_network.get_landmarks_from_image(a[i].permute(1, 2, 0))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["landmarks[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Disable spv input\n","spw_dims = [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 512, 512, 256, 256, 128, 128, 64]\n","spv = []\n","for i in spw_dims:\n","    spv.append(torch.cat((torch.ones((10, int(i / 2))), torch.zeros((10, int(i/2)))), dim=1))\n","spv = torch.cat(spv, dim=1).to(DEVICE)\n","\n","generated_images = style_generator.synthesis(ws, spv).detach()\n","generated_images = (generated_images * 127.5 + 128).clamp(0, 255).to(torch.uint8)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_landmarks = fa_network.get_landmarks_from_image(generated_images[0].permute(1,2,0), return_bboxes=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["landmark_acc_loss = 0\n","bbox_acc_loss = 0\n","for i in range(10):\n","    pred_ = fa_network.get_landmarks_from_image(generated_images[i].permute(1, 2, 0), return_bboxes=True)\n","    if len(pred_[0][0]) == 68:\n","        top = pred_[2][0][1]\n","        left = pred_[2][0][0]\n","        width = pred_[2][0][2] - pred_[2][0][0]\n","        height = pred_[2][0][3] - pred_[2][0][1]\n","\n","        pred_landmark = torch.from_numpy(pred_[0][0]).requires_grad_() - torch.tensor([left, top])\n","        pred_landmark /= torch.tensor([width, height])\n","\n","        landmark_acc_loss += landmarks[i].pow(2) - pred_landmark.view(-1).to(DEVICE).pow(2)\n","\n","        bbox_acc_loss += bboxs[i] - torch.tensor([top, left, width, height]).to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(train_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["landmark_acc_loss.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bbox_acc_loss.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if isinstance(landmark_acc_loss, torch.Tensor) and isinstance(bbox_acc_loss, torch.Tensor):\n","    pred_train_step_loss = landmark_acc_loss.mean() + bbox_acc_loss.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_train_step_loss.item()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["show_landmarks(generated_images[0].detach().cpu().permute(1,2,0), pred_landmarks[0][0], pred_landmarks[2], retuire_bbox=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top = pred_landmarks[2][0][1]\n","left = pred_landmarks[2][0][0]\n","width = pred_landmarks[2][0][2] - pred_landmarks[2][0][0]\n","height = pred_landmarks[2][0][3] - pred_landmarks[2][0][1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_lanmark = torch.tensor(pred_landmarks[0][0]) - torch.tensor([left, top])\n","pred_lanmark = pred_lanmark / torch.tensor([width, height])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss = landmarks[0].pow(2) - pred_lanmark.view(-1).pow(2).to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bboxs[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bbox_acc_loss = bboxs[0] - torch.tensor([top, left, width, height]).to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bbox_acc_loss.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_lanmark.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["original_style = []\n","modified_style = []\n","for block in style_generator.synthesis.children():\n","    for layer in block.children():\n","        if not isinstance(layer, networks.ToRGBLayer):\n","            original_style.append(layer.original_style)\n","            modified_style.append(layer.modified_style)\n","\n","style_ori = torch.cat(original_style, dim=1)\n","style_mod = torch.cat(modified_style, dim=1)\n","\n","score_ori = style_space_discriminator(style_ori)\n","score_mod = style_space_discriminator(style_mod)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spw = torch.randn(10, 12096)\n","spw_dims = [512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 256, 256, 128, 128, 64, 64, 32]\n","spw_dims = [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 512, 512, 256, 256, 128, 128, 64]\n","#spw_dims = [512, 1024, 1024, 1024, 1024, 768, 384, 192, 96]\n","\n","spw_idx = 0\n","spw_ = []\n","spw_.append(spw.narrow(1, spw_idx, spw_dims[0]))\n","spw_idx += spw_dims[0]\n","for idx in range(1, len(spw_dims), 2):\n","    spw_1 = spw.narrow(1, spw_idx, spw_dims[idx])\n","    spw_idx += spw_dims[idx]\n","    spw_2 = spw.narrow(1, spw_idx, spw_dims[idx+1])\n","    spw_idx += spw_dims[idx+1]\n","    spw_.append((spw_1, spw_2))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in spw_:\n","    if isinstance(i, tuple):\n","        print(i[0].shape, i[1].shape)\n","    else:\n","        print(i.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import PIL.Image as Image\n","\n","a = TF.to_pil_image(images[0])\n","b = TF.to_pil_image(generated_image[0])\n","\n","out_img = Image.new(\"RGB\", (512, 256))\n","\n","out_img.paste(a, (0, 0))\n","\n","out_img.paste(b, (256, 0))\n","\n","out_img.save(\"./output/saved_img/01.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred_results[2][0][0]"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"test.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.13 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"5b51819115219cd40fd69b2df748b382d358133772bdb6c26837ad86c4ab7674"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"02e82bcb367b44a68426232b081f54ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b314223ae7a4bd7b357aaceaeceb8b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9cd8b38eb8f74b229605ce491619df5c","placeholder":"​","style":"IPY_MODEL_ef13ae4c407e49698af9b626775caaa9","value":"100%"}},"15c9d74d91944a55ad55289ba58c17f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3746ffd983964757bb66506f083635b8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46e4393822e24443968cc7f49f7662af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b3e9080769e4382ac547252aebc58cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d44e163958142909c8e471fd834849b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b459f55dbf6242adb9ab6e34056ddd95","IPY_MODEL_6fee7f215d4b494293a1d3df2f23f8c5","IPY_MODEL_9e6b0a0596694cbab57ed3d467aeb4a5"],"layout":"IPY_MODEL_e1b9c956b0ed42a9a77095ef26cfe984"}},"5a7ab5ce05954f80b71f5f15db5123dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b314223ae7a4bd7b357aaceaeceb8b9","IPY_MODEL_ed082bd064b94ebabe1d671b9fb057bd","IPY_MODEL_80dd956bc36d456791527059d58e769c"],"layout":"IPY_MODEL_849a9830b9834dbf965700eb739bb019"}},"5caf9755fac04091ba4443ecf9d02b0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fee7f215d4b494293a1d3df2f23f8c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3746ffd983964757bb66506f083635b8","max":96316515,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce18ba7d42284881a1b0193b52349d58","value":96316515}},"80dd956bc36d456791527059d58e769c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b3e9080769e4382ac547252aebc58cf","placeholder":"​","style":"IPY_MODEL_02e82bcb367b44a68426232b081f54ca","value":" 85.7M/85.7M [00:08&lt;00:00, 12.6MB/s]"}},"831a7ab9ba114f04b7fafee3d14befe0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"849a9830b9834dbf965700eb739bb019":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cd8b38eb8f74b229605ce491619df5c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e6b0a0596694cbab57ed3d467aeb4a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15c9d74d91944a55ad55289ba58c17f8","placeholder":"​","style":"IPY_MODEL_5caf9755fac04091ba4443ecf9d02b0e","value":" 91.9M/91.9M [00:09&lt;00:00, 12.2MB/s]"}},"b459f55dbf6242adb9ab6e34056ddd95":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb6ff8b183724ce9bdef5faedd386a8d","placeholder":"​","style":"IPY_MODEL_e4620b5053c44d918b913e48d8de218b","value":"100%"}},"ce18ba7d42284881a1b0193b52349d58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1b9c956b0ed42a9a77095ef26cfe984":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4620b5053c44d918b913e48d8de218b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb6ff8b183724ce9bdef5faedd386a8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed082bd064b94ebabe1d671b9fb057bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_831a7ab9ba114f04b7fafee3d14befe0","max":89843225,"min":0,"orientation":"horizontal","style":"IPY_MODEL_46e4393822e24443968cc7f49f7662af","value":89843225}},"ef13ae4c407e49698af9b626775caaa9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
